---
title: 强化学习玩雀魂(1)
createTime: 2025/09/17 09:47:56
permalink: /article/i4jykx12/
---
本文总结了一下[视频](https://www.bilibili.com/video/BV1oT42167R6/) 中所展示的一些问题

视频是对北京大学组织的一个麻将(国标)比赛的总结,网站是[这里](botzone.org.cn)
<!-- more -->

## 麻将算法的困难

1. 是非完全信息博弈, 只能看到自己的牌和打出的牌, 其他玩家的牌是隐藏的
2. 随机性强, 初使的手牌和每一轮的发牌都是随机的, 这样会引入很大的方差
3. 价值估计的难度高, 不像扑克, 每张牌是有一个相对固定的价值的, 麻将之间则没有一个固定的大小关系, 手牌价值取决于各种胡牌番型的距离
4. 多目标决策, 麻将的胡牌是各种各样的, 要考虑游戏的进程, 以及其他玩家的行为, 进行一个动态的选择

## 算法选取

目前比较成功的麻将 AI 有微软的 Suphx 和 腾讯的 MJOLNIR

对于 微软的 Suphx, 其用了很多的小 trick, 不同的策略加了不少不同的层, 很多不平凡的设计

对于腾讯, 用的和 PPO 很类似, 但是双人麻将和四人麻将差别还是挺大的

### 平台介绍

采用的是北大的对战平台, 由于比赛的随机性, 所以对于同样的牌山, 会进行 4! 次轮换, 这样可以减少随机性的影响

### 比赛情况

在算法上, 大体可分为三类

#### 启发式算法
启发式算法, 主要是基于规则的, 通过一些启发式的规则来进行决策
  - 用向听数表示要替换的最少张数, 但可能不够 8 番起胡
  - 用搜索算法来尝试所有的可能, 但离太远时计算量很大
  - 在胡牌较远时, 用人工经验选方向

#### 监督学习

监督学习, 通过模仿现有的数据集来进行决策, 直接对每个局面预测动作

这样的算力要求就很低了, 但很难超过数据集的水平

第一届数据集很差, 所以监督学习的水平不是很高, 但还是超过了所有的启发式算法

#### 特征处理

##### 标量化

最简单的方法, 把局面信息转为一个标量, 编码到数组中

##### 图像化

把局面信息转为一个图像, 通过卷积神经网络来进行处理

比如 34张牌*重复张数4

也可以 4种花色*9张 * 重复张数 4

#### 网络结构

使用较深的网络来处理图像特征, 大部份使用的是 ResNet, 实验表明卷积网络的较果比全连接处理标量特征的表达能力更强

#### 数据增强

把1-9万, 1-9条, 1-9筒, 东南西北中发白 进行对称变换, 这样可以增加数据量

也可以把1-9换成9-1, 这样也不影响策略

这里就是我想改进的地方, 不是数据增强, 而是学习这些变换, 但最大的问题还是怎么表示差距呢

#### 强化学习
3. 强化学习, 通过自我对战来进行决策

在比赛中, 监督学习逐渐成为主流, 主要是其速度快, 资源少, 强化学习由于其大量的计算资源, 高校还是难以承受的

第一届中前三名是 腾讯, 网易 和 快手

采用的算法架构是一致的

1. 用的 IMPALA 架构, 一个 learner 训练多个 actor 采样对局, 用样本池传输样本
2. 共用的 backbone, 输出 policy head 和 value head(区别现 clanrl 和 ppo)
3. PPO, loss 有 Policy, value, entropy
4. 只用最新的模型自我对战, 不维护历史

有的会预训练, 由于比赛的随机性, 对局末的得分进行一个归一化, 还有的进行一些CTDE, 把策略和价值分开, 价值用的是全局信息输入, 策略用的是局部信息输入, 可以更准确的估计价值

> [!NOTE]
> 23 年的国科大的经验表明, 但算力不足的情况下, DQN 算法会比 PPO 更好
> 主要是因为 DQN 要更稳定, 从零开录的PPO 有一些番种会从来不做, 不从零开始的也会丢失一些番种能力, PPO<DQN<监督学习

## 有待改进的地方

怎么评价智能体的好坏, 麻将有随机性, 不能单纯的看胜率

其次的不稳定, 很多番种没有学会
