import{_ as i,c as p,b as t,a as o,d as a,e as r,w as s,r as l,o as h}from"./app-CN1iVX_s.js";const d={};function c(m,e){const n=l("VPLink");return h(),p("div",null,[e[2]||(e[2]=t("p",null,[a("本文总结了一下"),t("a",{href:"https://www.bilibili.com/video/BV1oT42167R6/",target:"_blank",rel:"noopener noreferrer"},"视频"),a(" 中所展示的一些问题")],-1)),t("p",null,[e[1]||(e[1]=a("视频是对北京大学组织的一个麻将(国标)比赛的总结,网站是",-1)),r(n,{href:"botzone.org.cn"},{default:s(()=>[...e[0]||(e[0]=[a("这里",-1)])]),_:1})]),e[3]||(e[3]=o('<h2 id="麻将算法的困难" tabindex="-1"><a class="header-anchor" href="#麻将算法的困难"><span>麻将算法的困难</span></a></h2><ol><li>是非完全信息博弈, 只能看到自己的牌和打出的牌, 其他玩家的牌是隐藏的</li><li>随机性强, 初使的手牌和每一轮的发牌都是随机的, 这样会引入很大的方差</li><li>价值估计的难度高, 不像扑克, 每张牌是有一个相对固定的价值的, 麻将之间则没有一个固定的大小关系, 手牌价值取决于各种胡牌番型的距离</li><li>多目标决策, 麻将的胡牌是各种各样的, 要考虑游戏的进程, 以及其他玩家的行为, 进行一个动态的选择</li></ol><h2 id="算法选取" tabindex="-1"><a class="header-anchor" href="#算法选取"><span>算法选取</span></a></h2><p>目前比较成功的麻将 AI 有微软的 Suphx 和 腾讯的 MJOLNIR</p><p>对于 微软的 Suphx, 其用了很多的小 trick, 不同的策略加了不少不同的层, 很多不平凡的设计</p><p>对于腾讯, 用的和 PPO 很类似, 但是双人麻将和四人麻将差别还是挺大的</p><h3 id="平台介绍" tabindex="-1"><a class="header-anchor" href="#平台介绍"><span>平台介绍</span></a></h3><p>采用的是北大的对战平台, 由于比赛的随机性, 所以对于同样的牌山, 会进行 4! 次轮换, 这样可以减少随机性的影响</p><h3 id="比赛情况" tabindex="-1"><a class="header-anchor" href="#比赛情况"><span>比赛情况</span></a></h3><p>在算法上, 大体可分为三类</p><h4 id="启发式算法" tabindex="-1"><a class="header-anchor" href="#启发式算法"><span>启发式算法</span></a></h4><p>启发式算法, 主要是基于规则的, 通过一些启发式的规则来进行决策</p><ul><li>用向听数表示要替换的最少张数, 但可能不够 8 番起胡</li><li>用搜索算法来尝试所有的可能, 但离太远时计算量很大</li><li>在胡牌较远时, 用人工经验选方向</li></ul><h4 id="监督学习" tabindex="-1"><a class="header-anchor" href="#监督学习"><span>监督学习</span></a></h4><p>监督学习, 通过模仿现有的数据集来进行决策, 直接对每个局面预测动作</p><p>这样的算力要求就很低了, 但很难超过数据集的水平</p><p>第一届数据集很差, 所以监督学习的水平不是很高, 但还是超过了所有的启发式算法</p><h4 id="特征处理" tabindex="-1"><a class="header-anchor" href="#特征处理"><span>特征处理</span></a></h4><h5 id="标量化" tabindex="-1"><a class="header-anchor" href="#标量化"><span>标量化</span></a></h5><p>最简单的方法, 把局面信息转为一个标量, 编码到数组中</p><h5 id="图像化" tabindex="-1"><a class="header-anchor" href="#图像化"><span>图像化</span></a></h5><p>把局面信息转为一个图像, 通过卷积神经网络来进行处理</p><p>比如 34张牌*重复张数4</p><p>也可以 4种花色*9张 * 重复张数 4</p><h4 id="网络结构" tabindex="-1"><a class="header-anchor" href="#网络结构"><span>网络结构</span></a></h4><p>使用较深的网络来处理图像特征, 大部份使用的是 ResNet, 实验表明卷积网络的较果比全连接处理标量特征的表达能力更强</p><h4 id="数据增强" tabindex="-1"><a class="header-anchor" href="#数据增强"><span>数据增强</span></a></h4><p>把1-9万, 1-9条, 1-9筒, 东南西北中发白 进行对称变换, 这样可以增加数据量</p><p>也可以把1-9换成9-1, 这样也不影响策略</p><p>这里就是我想改进的地方, 不是数据增强, 而是学习这些变换, 但最大的问题还是怎么表示差距呢</p><h4 id="强化学习" tabindex="-1"><a class="header-anchor" href="#强化学习"><span>强化学习</span></a></h4><ol start="3"><li>强化学习, 通过自我对战来进行决策</li></ol><p>在比赛中, 监督学习逐渐成为主流, 主要是其速度快, 资源少, 强化学习由于其大量的计算资源, 高校还是难以承受的</p><p>第一届中前三名是 腾讯, 网易 和 快手</p><p>采用的算法架构是一致的</p><ol><li>用的 IMPALA 架构, 一个 learner 训练多个 actor 采样对局, 用样本池传输样本</li><li>共用的 backbone, 输出 policy head 和 value head(区别现 clanrl 和 ppo)</li><li>PPO, loss 有 Policy, value, entropy</li><li>只用最新的模型自我对战, 不维护历史</li></ol><p>有的会预训练, 由于比赛的随机性, 对局末的得分进行一个归一化, 还有的进行一些CTDE, 把策略和价值分开, 价值用的是全局信息输入, 策略用的是局部信息输入, 可以更准确的估计价值</p><div class="hint-container note"><p class="hint-container-title">注</p><p>23 年的国科大的经验表明, 但算力不足的情况下, DQN 算法会比 PPO 更好 主要是因为 DQN 要更稳定, 从零开录的PPO 有一些番种会从来不做, 不从零开始的也会丢失一些番种能力, PPO&lt;DQN&lt;监督学习</p></div><h2 id="有待改进的地方" tabindex="-1"><a class="header-anchor" href="#有待改进的地方"><span>有待改进的地方</span></a></h2><p>怎么评价智能体的好坏, 麻将有随机性, 不能单纯的看胜率</p><p>其次的不稳定, 很多番种没有学会</p>',41))])}const x=i(d,[["render",c]]),u=JSON.parse('{"path":"/article/i4jykx12/","title":"强化学习玩雀魂(1)","lang":"zh-CN","frontmatter":{"title":"强化学习玩雀魂(1)","createTime":"2025/09/17 09:47:56","permalink":"/article/i4jykx12/","description":"本文总结了一下视频 中所展示的一些问题 视频是对北京大学组织的一个麻将(国标)比赛的总结,网站是 麻将算法的困难 是非完全信息博弈, 只能看到自己的牌和打出的牌, 其他玩家的牌是隐藏的 随机性强, 初使的手牌和每一轮的发牌都是随机的, 这样会引入很大的方差 价值估计的难度高, 不像扑克, 每张牌是有一个相对固定的价值的, 麻将之间则没有一个固定的大小关...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"强化学习玩雀魂(1)\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-17T03:48:27.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://shinonomeow.top/article/i4jykx12/"}],["meta",{"property":"og:site_name","content":"東雲研究所"}],["meta",{"property":"og:title","content":"强化学习玩雀魂(1)"}],["meta",{"property":"og:description","content":"本文总结了一下视频 中所展示的一些问题 视频是对北京大学组织的一个麻将(国标)比赛的总结,网站是 麻将算法的困难 是非完全信息博弈, 只能看到自己的牌和打出的牌, 其他玩家的牌是隐藏的 随机性强, 初使的手牌和每一轮的发牌都是随机的, 这样会引入很大的方差 价值估计的难度高, 不像扑克, 每张牌是有一个相对固定的价值的, 麻将之间则没有一个固定的大小关..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-09-17T03:48:27.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-17T03:48:27.000Z"}]]},"readingTime":{"minutes":3.88,"words":1164},"git":{"createdTime":1758080907000,"updatedTime":1758080907000,"contributors":[{"name":"shinonomeow","username":"shinonomeow","email":"gtx2shino@gmail.com","commits":1,"avatar":"https://avatars.githubusercontent.com/shinonomeow?v=4","url":"https://github.com/shinonomeow"}]},"autoDesc":true,"filePathRelative":"learning_note/强化学习玩雀魂(1).md","headers":[],"categoryList":[{"id":"987a5d","sort":10001,"name":"learning_note"}]}');export{x as comp,u as data};
